# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15ngUsNW7l59RxIS96_e9_qB-GzHq2s6Q
"""

# ==========================================
# üáµüá± Pe≈Çny notebook - Trening HerBERT na PolEmo
# ==========================================

#  Wy≈ÇƒÖczenie logowania do W&B i HuggingFace
import os
os.environ["WANDB_DISABLED"] = "true"
os.environ["HF_HUB_DISABLE_TELEMETRY"] = "1"

# ==========================================
#  1. Instalacja stabilnych bibliotek
# ==========================================
!pip uninstall -y transformers torch torchaudio torchvision pyarrow -q
!pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --quiet
!pip install transformers==4.44.2 datasets==2.19.1 scikit-learn sacremoses accelerate --quiet

import torch
print("GPU:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU only")

# ==========================================
#  2. Importy
# ==========================================
import pandas as pd
import numpy as np
from datasets import Dataset
from sklearn.model_selection import train_test_split
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    pipeline
)
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from google.colab import files

print(" Biblioteki za≈Çadowane")

# ==========================================
#  3. Wczytanie danych
# ==========================================
print("\n Wybierz plik polemo_clean.csv z projektu sentiment_app/data/")
uploaded = files.upload()
file_name = list(uploaded.keys())[0]

df = pd.read_csv(file_name)
df = df.dropna(subset=["text", "label"])
print(f" Wczytano {len(df)} rekord√≥w\n")
print(df["label"].value_counts())

# ==========================================
#  4. Mapowanie etykiet
# ==========================================
label2id = {"negatywny": 0, "neutralny": 1, "pozytywny": 2}
id2label = {v: k for k, v in label2id.items()}
df["label_id"] = df["label"].map(label2id)

# ==========================================
#  5. Podzia≈Ç danych
# ==========================================
train_df, test_df = train_test_split(df, test_size=0.2, stratify=df["label_id"], random_state=42)
train_dataset = Dataset.from_pandas(train_df[["text", "label_id"]])
test_dataset = Dataset.from_pandas(test_df[["text", "label_id"]])

# ==========================================
#  6. Tokenizer i model HerBERT
# ==========================================
MODEL_NAME = "allegro/herbert-base-cased"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

def tokenize(batch):
    return tokenizer(batch["text"], padding="max_length", truncation=True, max_length=128)

train_dataset = train_dataset.map(tokenize, batched=True)
test_dataset = test_dataset.map(tokenize, batched=True)

# ‚úÖ poprawka: Trainer wymaga kolumny "labels"
train_dataset = train_dataset.rename_column("label_id", "labels")
test_dataset = test_dataset.rename_column("label_id", "labels")

model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,
    id2label=id2label,
    label2id=label2id
)

# ==========================================
#  7. Metryki
# ==========================================
def compute_metrics(pred):
    labels = pred.label_ids
    preds = np.argmax(pred.predictions, axis=1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average="weighted")
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "f1": f1, "precision": precision, "recall": recall}

# ==========================================
#  8. Parametry treningu
# ==========================================
training_args = TrainingArguments(
    output_dir="./herbert_results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=1,   # Mo≈ºesz zwiƒôkszyƒá np. do 2-3 dla lepszych wynik√≥w
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=100,
    report_to="none",  # wy≈ÇƒÖcza W&B
    load_best_model_at_end=True
)

# ==========================================
#  9. Trening
# ==========================================
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

print("\n Rozpoczynam trening HerBERT-a...")
trainer.train()
print(" Trening zako≈Ñczony!")

# ==========================================
#  10. Ewaluacja
# ==========================================
print("\n Wyniki na zbiorze testowym:")
results = trainer.evaluate()
print(results)

# ==========================================
#  11. Zapis modelu
# ==========================================
SAVE_DIR = "./herbert_sentiment"
model.save_pretrained(SAVE_DIR)
tokenizer.save_pretrained(SAVE_DIR)
print(f" Model zapisano w {SAVE_DIR}")

# ==========================================
#  12. Spakowanie modelu
# ==========================================
!zip -r herbert_sentiment.zip herbert_sentiment
files.download("herbert_sentiment.zip")

# ==========================================
#  13. Test predykcji
# ==========================================
pipe = pipeline("text-classification", model=SAVE_DIR, tokenizer=SAVE_DIR, return_all_scores=True)
test_text = "Obs≈Çuga by≈Ça szybka i bardzo mi≈Ça!"
print("\n Predykcja dla:", test_text)
print(pipe(test_text))

# ==========================================
# üìä DODATKOWA EWALUACJA: classification_report + confusion_matrix
# ==========================================
from sklearn.metrics import classification_report, confusion_matrix

# 1. Pobierz predykcje na zbiorze testowym
predictions = trainer.predict(test_dataset)
y_true = predictions.label_ids
y_pred = np.argmax(predictions.predictions, axis=1)

# 2. Nazwy klas (etykiet)
labels_names = [id2label[i] for i in range(len(id2label))]
print("Etykiety klas:", labels_names)

# 3. Pe≈Çny raport klasyfikacji
print("\n=== Raport klasyfikacji (HerBERT) ===")
print(classification_report(
    y_true,
    y_pred,
    target_names=labels_names,
    digits=3
))

# 4. Macierz pomy≈Çek
print("=== Macierz pomy≈Çek (HerBERT) ===")
print(confusion_matrix(y_true, y_pred))

